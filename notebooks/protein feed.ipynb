{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0763bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded features ((870, 42)) and target (      protein feed\n",
      "834              1\n",
      "552            -35\n",
      "855             -2\n",
      "215            -10\n",
      "256             10\n",
      "...            ...\n",
      "330            -10\n",
      "466            -19\n",
      "121              3\n",
      "1044            -8\n",
      "860             13\n",
      "\n",
      "[870 rows x 1 columns]).\n",
      "------------------------------------------------------------\n",
      "Starting Grid Search CV and training of 6 individual SVR models...\n",
      "------------------------------------------------------------\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " üèÜ Best Parameters Found: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      " üéØ (Test RMSE: 3.3498)\n",
      "------------------------------------------------------------\n",
      "Grid Search CV complete. The best SVR model is trained and ready for export.\n",
      "‚úÖ Trained model saved to protein_feed.pkl. Ready for prediction.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib \n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Load Processed Training Data ---\n",
    "\n",
    "try:\n",
    "    x_train = joblib.load('../models/x_train.pkl')\n",
    "    x_test=joblib.load('../models/x_test.pkl')\n",
    "    y_train=joblib.load('../models/y_train.pkl')\n",
    "    y_train_p=y_train[['protein feed']]\n",
    "    y_test=joblib.load('../models/y_test.pkl')\n",
    "    y_test_p=y_test[['protein feed']]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"-\" * 60)\n",
    "    print(\"FATAL ERROR: Processed data files not found.\")\n",
    "    print(\"Ensure the pipeline_exporter script has been run successfully to create these PKL files.\")\n",
    "    print(\"-\" * 60)\n",
    "    raise\n",
    "\n",
    "print(f\"‚úÖ Successfully loaded features ({x_train.shape}) and target ({y_train_p}).\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# --- 2. Initialize, Train, and Export Model ---\n",
    "\n",
    "# GRID SEARCH SVM\n",
    "import joblib \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV # <--- NEW IMPORT\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- 1. Define the Parameter Grid for Grid Search ---\n",
    "# This dictionary holds the hyperparameters and the values you want to test.\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.1, 1], # Kernel coefficient\n",
    "    'kernel': ['rbf'] # For SVR, 'rbf' is commonly used\n",
    "}\n",
    "\n",
    "# --- 2. Define the Base Model ---\n",
    "base_model = SVR()\n",
    "\n",
    "# --- 3. Grid Search and Training Loop ---\n",
    "print(\"Starting Grid Search CV and training of 6 individual SVR models...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# NOTE: Since your original snippet only shows training for a single model\n",
    "# (using x_train and y_train_p), the example below simulates the loop structure\n",
    "# you would need for all six targets.\n",
    "\n",
    "# Assuming you are iterating through targets like this:\n",
    "# for i, target_col in enumerate(TARGET_COLUMNS):\n",
    "#     y_train_p = Y_for_fit_train.iloc[:, i] \n",
    "#     y_test_p = Y_for_fit_test.iloc[:, i]\n",
    "#     ...\n",
    "\n",
    "# --- Example for a single target (as in your original snippet) ---\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "# cv=5 means 5-fold cross-validation\n",
    "# scoring='neg_mean_squared_error' is common for regression (it maximizes the negative MSE, \n",
    "# which is equivalent to minimizing the positive MSE/RMSE)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5, \n",
    "    verbose=1,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit Grid Search to the training data\n",
    "# This step performs the cross-validation across all parameter combinations\n",
    "grid_search.fit(x_train, y_train_p) \n",
    "\n",
    "# The best model found by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\" üèÜ Best Parameters Found: {best_params}\")\n",
    "\n",
    "# Optional: Quick Evaluation on Testing Data (for confidence)\n",
    "y_pred_p = best_model.predict(x_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_p, y_pred_p))\n",
    "    \n",
    "print(f\" üéØ (Test RMSE: {rmse:.4f})\")\n",
    "\n",
    "# Export the best model (e.g., using a target-specific filename)\n",
    "# model_filename = f'svr_model_{target_col}.pkl'\n",
    "# joblib.dump(best_model, model_filename)\n",
    "# print(f\" üíæ Model saved as {model_filename}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Grid Search CV complete. The best SVR model is trained and ready for export.\")\n",
    "\n",
    "# C. Export the Fitted Model\n",
    "# This is the file you need to upload as 'protein.pkl' (or rename it after export)\n",
    "joblib.dump(best_model, 'protein_feed.pkl')\n",
    "\n",
    "print(\"‚úÖ Trained model saved to protein_feed.pkl. Ready for prediction.\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83acdd73-5912-431c-8b95-68c6be2470ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded '../models/preprocessor.pkl' and '../models/protein_feed.pkl'.\n",
      "\n",
      "Starting Validation Flow:\n",
      "------------------------------\n",
      "Raw Input: ['CM3-A' 'Yes' 712 '2080' 'Level 1']\n",
      "------------------------------\n",
      "1. Preprocessing complete. Output shape: (1, 42)\n",
      "   (First 5 processed values: [1. 1. 0. 0. 1.])\n",
      "\n",
      "--- Final Prediction Result ---\n",
      "Predicted change in 'Protein Feed' yield: -0.10 %\n",
      "-------------------------------\n",
      "Conclusion: The pipeline successfully transformed new data and generated a prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration (Names of your deployed assets) ---\n",
    "PREPROCESSOR_FILE = '../models/preprocessor.pkl'\n",
    "MODEL_FILE = '../models/protein_feed.pkl' # Assuming this is your final SVR model file\n",
    "\n",
    "# --- 1. Define Example Raw Input Data ---\n",
    "# This dictionary must contain all the original feature columns \n",
    "# in the format they would arrive from a web form or database.\n",
    "example_raw_input = pd.DataFrame({\n",
    "    'BLS Code': [41],             # Example BLS Code\n",
    "    'Scenario': ['CM3-A'],        # Must be a category seen in training\n",
    "    'Time_Slice': ['2080'],       # Needs scaling\n",
    "    'CO2 effects': ['Yes'],\n",
    "    'CO2 ppm': [712],             # Needs scaling\n",
    "    'Adaptation': ['Level 1'],\n",
    "    # Note: Other columns like 'wheat', 'rice', etc., are targets and NOT included here.\n",
    "})\n",
    "\n",
    "# --- 2. Load the Deployed Assets ---\n",
    "try:\n",
    "    preprocessor = joblib.load(PREPROCESSOR_FILE)\n",
    "    model_protein_feed = joblib.load(MODEL_FILE)\n",
    "    print(f\"‚úÖ Successfully loaded '{PREPROCESSOR_FILE}' and '{MODEL_FILE}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"-\" * 60)\n",
    "    print(\"FATAL ERROR: Could not find one or more PKL files.\")\n",
    "    print(\"Please ensure your pipeline and model export steps ran successfully.\")\n",
    "    print(\"-\" * 60)\n",
    "    raise\n",
    "\n",
    "# --- 3. Execute the Full Prediction Flow ---\n",
    "\n",
    "print(\"\\nStarting Validation Flow:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Raw Input: {example_raw_input[['Scenario', 'CO2 effects', 'CO2 ppm', 'Time_Slice', 'Adaptation']].values[0]}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# A. Preprocessing: Transform the raw data (9 features) into the clean format (43 features)\n",
    "# This is the crucial step where the preprocessor checks if it's running well.\n",
    "X_new_processed_array = preprocessor.transform(example_raw_input)\n",
    "\n",
    "# Optional: Inspect the processed output (helps debugging order/scaling issues)\n",
    "X_new_processed_df = pd.DataFrame(\n",
    "    X_new_processed_array, \n",
    "    columns=preprocessor.named_steps['preprocessor'].get_feature_names_out()\n",
    ")\n",
    "print(f\"1. Preprocessing complete. Output shape: {X_new_processed_array.shape}\")\n",
    "print(f\"   (First 5 processed values: {X_new_processed_array[0][:5]})\")\n",
    "\n",
    "\n",
    "# B. Prediction: Pass the cleaned array to the fitted SVR model\n",
    "prediction = model_protein_feed.predict(X_new_processed_array)\n",
    "\n",
    "print(\"\\n--- Final Prediction Result ---\")\n",
    "print(f\"Predicted change in 'Protein Feed' yield: {prediction[0]:.2f} %\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# --- Verification of Pipeline Health ---\n",
    "# If the code runs without an error (especially KeyError or ValueError during transform), \n",
    "# the pipeline is healthy and ready for deployment.\n",
    "print(\"Conclusion: The pipeline successfully transformed new data and generated a prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43cfa2-836c-4f0d-9ea7-8eaa4db88534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
